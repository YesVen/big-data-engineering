What is streaming access pattern?
Write once and read any number of times but don't try to change the content of the file once you are keeping in the HDFS.

On top of Hard disk we are creating HDFS. It is built on top of normal file system.
Default Block size is 64MB
So for processing huge datasets Haoop is using 64MB block size and not default block size of 4Kb on hard disk.

**
Extra: Hard disk consists of blocks on one sector and one block is 4KB. There are several sectorsin Hard disk. 
If i try to save 2 KB of file than 2 KB will be wasted.
Hadoop Administrator should take care of this HDFS and its configuration and not developers.
**
So if you store a file of size 35MB than 29MB won't be wasted by Hadoop but it will be released for other files.
Eg: for 500 GB hard disk, 7500 blocks will be given.
-----------------------------------------------------------------------------------------------------------------

Hadoop Architecture

Master Services: They can talk to each other. Eg: NN can talk to SNN, JT. NN can talk to it's corresponding DN and vice versa but not to TT.
Name node,(NN)
Secondary Name node,(SNN)
Job Tracker(JT)

Slave Services: Similarly, TT can talk to JT and vice versa. TT can talk to DN.
Data node,(DN)
Task Tracker(TT)

------------------------------------------------------------------------------------------------------------------

Example scenario
