https://www.youtube.com/watch?v=DLutRT6K2rM
What is streaming access pattern?
Write once and read any number of times but don't try to change the content of the file once you are keeping in the HDFS.

On top of Hard disk we are creating HDFS. It is built on top of normal file system.
Default Block size is 64MB in Hadoop
So for processing huge datasets Hadoop is using 64MB block size and not default block size of 4Kb on hard disk.

**
Extra: Hard disk consist of blocks on one sector and one block is 4KB. There are several sectors in Hard disk. 
If i try to save 2 KB of file than other 2 KB will be wasted.
Hadoop Administrator should take care of this HDFS and its configuration and not developers.
**
So if you store a file of size 35MB than 29MB won't be wasted by Hadoop but it will be released for other files.
Eg: for 500 GB hard disk, 7500 blocks will be given.
-----------------------------------------------------------------------------------------------------------------

Hadoop Architecture

Master Services: They can talk to each other. Eg: NN can talk to SNN, JT. NN can talk to it's corresponding DN and vice versa but not to TT.
Name node,(NN) -- It should be a highly reliable hardware.
Secondary Name node,(SNN)
Job Tracker(JT)

Slave Services: Similarly, TT can talk to JT and vice versa. TT can talk to DN.
Data node,(DN)
Task Tracker(TT)

------------------------------------------------------------------------------------------------------------------

Example scenario of how storage is done.
So we are going to store a file name File.txt 200 MB of file on Hadoop.
So 4 blocks will be given to this 200 MB file. 64*3 + 8MB = 200MB.
So NN will take care of splitting this 200MB of file into 64MB blocks.

So NN acts as a Manager and client sends a request to NN. It stores the metadata about the file name File.txt
1) NN receives the request and stores details in its meta data. 
   Details like 4 files will be created named a.txt, b.txt, c.txt --> 64MB 
   and d.txt --> 8MB.
2) So NN says to client that please go and store your file in DN number 1,3,5,7
   So now the clientvery well knows which DN are having free space. So now client is directly approaching these systems.
3) So now the client is approaching DN1 and keeping the a.txt file there.

**
Extra: So all these DN are cheap commodity hardwares, so every DN will be there with 300 or 500GB of HD capacity for example.
If it is 500Gb HD than 7500 blocks will be there.
So out of all these blocks it will try to keep this a.txt file in one of the 7500 blocks.
**
**
Extra: If DN fails, this HDFS has been given 3 replications bydefault.
So now 200MB of file will take 600MB space in total at 3 different DN. Like 200MB at each DN to achieve redundancy and backup.
TO overcome data loss problem.
**

4) So now DN1 will keep a.txt file in some other system(node) say DN2
   So now this DN2 will keep the same a.txt file in some other node say DN4(it can be any random node which has free space).
5) DN4 sends acknowledgement to DN2 and DN2 to DN1 and DN1 to client and 
   says(sends acknowledgement to) client that file a.txt has been stored to DN1 and DN2, DN4 also.
6) Now every DN will give proper Block_Report and Heart_Beat back to the NN for every short period of time.
   Block_Report: some client has stored some block in my local DN
   Heart_Beat: we are still alive, processing and working properly.
7) NN will log the system number for that block.
   For eg: Metadata will store that a.txt file is stored in DN1 and its replications are stored in DN2 and DN3.
   In the same way client keeps b.txt, c.txt and d.txt
   
**
Extra: So suppose at any point of time some DN1 stops sending HEART_BEAT to NN, i.e, DN is dead than NN thinks that DN1 is dead.
So NN removes the metatdata of DN1 from its hard disk. Basically, it is removing DN1's entry in metadata for a.txt file.
But now the replication factor is 3 so NN will choose some other DN(for ex. DN7) out of all these DNs and put/update the entry of metadata for a.txt file.
and than DN7 starts sending HEART_BEAT to NN.
**
**
Extra: What will happen if that metadata is going to be lost.?
If there is no metadata, there is no use with that hadoop. Your HDFS will never be working.
Metadata will be stored in NN's hard disk.
**

*****
Important: Why there is 64MB and not 4KB of block size?
Because if Block size is 4Kb than to store 200Mb of data, 50,000 .txt files will be created and NN will have to log
50,000 metadata records(So it has to create metadata for 50,000 blocks) which is large compared to 4 metadata records in 64MB case. And for 500GB only 7500 blocks are there.
In this way it occupies less memory in NN
So huge block sizes less metadata.

NN and JT: is also called a single point of failure (SPOF) is a part of a system that, if it fails, will stop the entire system from working. 
SPOFs are undesirable in any system with a goal of high availability or reliability
*****
-----------------------------------------------------------------------------------------------------------------------

Example scenario of processing the above stored data.
1) JobTracker(JT) will take the request(10Kb of program we wrote to query the data stored on hadoop).
   JT doesn't know what all DNs are having what blocks, because there is no communication between JT and DN.
2) JT is trying to send request to the NN and saying that, 
   "Some client wanted to process File.txt file and he has given me some program.
   I wanted to apply that program on File.txt file. I am unaware about what all blocks we are having and where does thse blocks reside.
   If you can give me those details from your metadata than i can start processing that data."
   Something like this JT will give request to the NN.
3) Now the NN will check it up that if the File.txt is there in his metadata or not.
   This metadata will be given to JT by NN. So now JT has all information about the File.txt.
   
   **
   Extra: JT role is to assign task to TT
   **
4) Now the JT will see where that a.txt file is available. It is avaible in DN1, DN2, DN4
   So it(JT) will send request to it's nearer DN so that request can be sent in less time.
5) Now this JT will assign(send request) this task to TT1. It will say that 
   "Hey TT please take this 10kb of program and start working/computing/processing on that."
   TT1 is applying that computation on a.txt file.
   
*****
Important: Applying that program on your local a.txt file is a process.This process we are calling it as MAP.
File.txt is input file and a.txt, b.txt, c.txt, d.txt are input splits.
So number of input splits is equal to those many MAPPERS are running in your cluster.
*****
*****
Important: Now when the TT(DN1) is processing and suddenly that DN is dead/disturbed/crashed than this TT(DN1) will just message the 
JT that he can't process the task. So now the JT will give same task to some other TT(DN3) who has that same data/same a.txt file in it's DN.
*****
*****
Important: How the JT(master) will know that TT(slaves) is dead?
So all these TTs will give HEART_BEAT to JT every 3 seconds to say that they are still alive and working properly.
So JT will wait for 10 HEART_BEATS (30 seconds). If no response than it means dead or working slowly.
*****
Important: When is the TT running really slow?
So sometimes there is 100s of request coming to the same TT(DN1) so the same TT will be running 100 MAPPERS at the same time.
So out of 7500 blocks, 100 clients are running 100 MAPPERS. It is processing 100 request so it is working slowly.
So it cannot give proper HEART_BEAT
So it can give task to DN which is far away.(Think like a LOAD_BALANCER thing)
*****
Important: Maintain a reliably good hardware system for JT because if it is down than all the Tasks will be disturbed and 
data won't be processed.

6) Now suppose a.txt is giving 4KB of output, b.txt 4KB, c.txt 3KB, d.txt 1KB. So this is giving 
   12KB of output for this File.txt(200MB)

REDUCER will come and combine all your output. By default, 1 reducer will be there.

*****
IMPORTANT: number of REDUCER is equal to number of number of output files.
           number of MAPPER is equal to number of input splits.
*****

Let's assume REDUCER will be there in DN8, than ouput will be given in that local DN itself.
7) So now when this DN8 is giving HEART_BEAT to the NN, it will say that
"Some client has processed this data and the output directory is in <TEST_OUTPUT> directory of DN8"
 NN will store the output directory and DN number in it's metadata.

8) When 100% processing is done, client will contact NN and NN will see in it's metadata that File.txt's output is at the above <TEST_OUTPUT> directory.
   So now it returns  info to client.
   Client now contacts DN8 and he will gets it output from there.









