What is streaming access pattern?
Write once and read any number of times but don't try to change the content of the file once you are keeping in the HDFS.

On top of Hard disk we are creating HDFS.
Default Block size is 64MB
So for processing huge datasets Haoop is using 64MB block size and not default block size of 4Kb on hard disk.

**
Extra: Hard disk consists of blocks on one sector and one block is 4KB. There are several sectorsin Hard disk. 
If i try to save 2 KB of file than 2 KB will be wasted.
Hadoop Administrator should take care of this HDFS and its configuration and not developers.
**
So if you store a file of size 35MB than 29MB won't be wasted by Hadoop but it will be released for other files.
Eg: for 500 GB hard disk, 7500 blocks will be given.
