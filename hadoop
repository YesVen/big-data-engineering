https://www.youtube.com/watch?v=DLutRT6K2rM
What is streaming access pattern?
Write once and read any number of times but don't try to change the content of the file once you are keeping in the HDFS.

On top of Hard disk we are creating HDFS. It is built on top of normal file system.
Default Block size is 64MB
So for processing huge datasets Haoop is using 64MB block size and not default block size of 4Kb on hard disk.

**
Extra: Hard disk consists of blocks on one sector and one block is 4KB. There are several sectorsin Hard disk. 
If i try to save 2 KB of file than 2 KB will be wasted.
Hadoop Administrator should take care of this HDFS and its configuration and not developers.
**
So if you store a file of size 35MB than 29MB won't be wasted by Hadoop but it will be released for other files.
Eg: for 500 GB hard disk, 7500 blocks will be given.
-----------------------------------------------------------------------------------------------------------------

Hadoop Architecture

Master Services: They can talk to each other. Eg: NN can talk to SNN, JT. NN can talk to it's corresponding DN and vice versa but not to TT.
Name node,(NN)
Secondary Name node,(SNN)
Job Tracker(JT)

Slave Services: Similarly, TT can talk to JT and vice versa. TT can talk to DN.
Data node,(DN)
Task Tracker(TT)

------------------------------------------------------------------------------------------------------------------

Example scenario
So we are going to store a file name file.txt 200 MB of file on Hadoop.
So 4 blocks will be given to this 200 MB file. 64*3 + 8MB = 200MB.
So NN will take care of splitting this 200MB of file into 64MB blocks.

So NN acts as a Manager and client sends a request to NN. It stores the metadata about the file name file.txt
1) NN receives the request and stores details in its meta data. 
   Details like 4 files will be created named a.txt, b.txt, c.txt --> 64MB 
   and d.txt --> 8MB.
2) So NN says to client that please go and store your file in DN number 1,3,5,7
   So now the clientvery well knows which DN are having free space. So now client is directly approaching these systems.
3) So now the client is approaching DN1 and keeping the a.txt file there.

**
Extra: So all these DN are cheap commodity hardwares, so every DN will be there with 300 or 500GB of HD capacity for example.
If it is 500Gb HD than 7500 blocks will be there.
So out of all these blocks it will try to keep this a.txt file in one of the 7500 blocks.
**
**
Extra: If DN fails, this HDFS has been given 3 replications bydefault.
So now 200MB of file will take 600MB space in total at 3 different DN. Like 200MB at each DN to achieve redundancy and backup.
TO overcome data loss problem.
**

4) So now DN1 will keep a.txt file in some other system(node) say DN2
   So now this DN2 will keep the same a.txt file in some other node say DN4(it can be any random node which has free space).
5) DN4 sends acknowledgement to DN2 and DN2 to DN1 and DN1 to client and 
   says(sends acknowledgement to) client that file a.txt has been stored to DN1 and DN2, DN4 also.
6) Now every DN will give proper Block_Report and Heart_Beat back to the NN for every short period of time.
   Block_Report: some client has stored some block in my local DN
   Heart_Beat: we are still alive, processing and working properly.
7) NN will log the system number for that block.
   For eg: Metadata will store that a.txt file is stored in DN1 and its replications are stored in DN2 and DN3.
   In the same way client keeps b.txt, c.txt and d.txt
